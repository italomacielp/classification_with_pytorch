{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Importação de bibliotecas"
      ],
      "metadata": {
        "id": "VD-A0fgrL1QO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wuXdJXGLxAj"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader, Dataset, random_split, WeightedRandomSampler\n",
        "from torchvision.transforms.v2 import Compose, Normalize, ToPILImage, Resize, ToDtype\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('fivethirtyeight')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classe arquitetura"
      ],
      "metadata": {
        "id": "i3ksV1-4L4pK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Architecture(object):\n",
        "    def __init__(self, model, loss_fn, optimizer):\n",
        "        # Here we define the attributes of our class\n",
        "\n",
        "        # We start by storing the arguments as attributes\n",
        "        # to use them later\n",
        "        self.model = model\n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        # Let's send the model to the specified device right away\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # These attributes are defined here, but since they are\n",
        "        # not informed at the moment of creation, we keep them None\n",
        "        self.train_loader = None\n",
        "        self.val_loader = None\n",
        "\n",
        "        # These attributes are going to be computed internally\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        self.total_epochs = 0\n",
        "\n",
        "        # Creates the train_step function for our model,\n",
        "        # loss function and optimizer\n",
        "        # Note: there are NO ARGS there! It makes use of the class\n",
        "        # attributes directly\n",
        "        self.train_step_fn = self._make_train_step_fn()\n",
        "        # Creates the val_step function for our model and loss\n",
        "        self.val_step_fn = self._make_val_step_fn()\n",
        "\n",
        "        # for hook purposes\n",
        "        self.handles = {}\n",
        "        self.visualization = {}\n",
        "\n",
        "    def to(self, device):\n",
        "        # This method allows the user to specify a different device\n",
        "        # It sets the corresponding attribute (to be used later in\n",
        "        # the mini-batches) and sends the model to the device\n",
        "        try:\n",
        "            self.device = device\n",
        "            self.model.to(self.device)\n",
        "        except RuntimeError:\n",
        "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "            print(f\"Couldn't send it to {device}, sending it to {self.device} instead.\")\n",
        "            self.model.to(self.device)\n",
        "\n",
        "    def set_loaders(self, train_loader, val_loader=None):\n",
        "        # This method allows the user to define which train_loader (and val_loader, optionally) to use\n",
        "        # Both loaders are then assigned to attributes of the class\n",
        "        # So they can be referred to later\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "\n",
        "    def _make_train_step_fn(self):\n",
        "        # This method does not need ARGS... it can refer to\n",
        "        # the attributes: self.model, self.loss_fn and self.optimizer\n",
        "\n",
        "        # Builds function that performs a step in the train loop\n",
        "        def perform_train_step_fn(x, y):\n",
        "            # Sets model to TRAIN mode\n",
        "            self.model.train()\n",
        "\n",
        "            # Step 1 - Computes our model's predicted output - forward pass\n",
        "            yhat = self.model(x)\n",
        "            # Step 2 - Computes the loss\n",
        "            loss = self.loss_fn(yhat, y)\n",
        "            # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n",
        "            loss.backward()\n",
        "            # Step 4 - Updates parameters using gradients and the learning rate\n",
        "            self.optimizer.step()\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Returns the loss\n",
        "            return loss.item()\n",
        "\n",
        "        # Returns the function that will be called inside the train loop\n",
        "        return perform_train_step_fn\n",
        "\n",
        "    def _make_val_step_fn(self):\n",
        "        # Builds function that performs a step in the validation loop\n",
        "        def perform_val_step_fn(x, y):\n",
        "            # Sets model to EVAL mode\n",
        "            self.model.eval()\n",
        "\n",
        "            # Step 1 - Computes our model's predicted output - forward pass\n",
        "            yhat = self.model(x)\n",
        "            # Step 2 - Computes the loss\n",
        "            loss = self.loss_fn(yhat, y)\n",
        "            # There is no need to compute Steps 3 and 4, since we don't update parameters during evaluation\n",
        "            return loss.item()\n",
        "\n",
        "        return perform_val_step_fn\n",
        "\n",
        "    def _mini_batch(self, validation=False):\n",
        "        # The mini-batch can be used with both loaders\n",
        "        # The argument `validation`defines which loader and\n",
        "        # corresponding step function is going to be used\n",
        "        if validation:\n",
        "            data_loader = self.val_loader\n",
        "            step_fn = self.val_step_fn\n",
        "        else:\n",
        "            data_loader = self.train_loader\n",
        "            step_fn = self.train_step_fn\n",
        "\n",
        "        if data_loader is None:\n",
        "            return None\n",
        "\n",
        "        # Once the data loader and step function, this is the same\n",
        "        # mini-batch loop we had before\n",
        "        mini_batch_losses = []\n",
        "        for x_batch, y_batch in data_loader:\n",
        "            x_batch = x_batch.to(self.device)\n",
        "            y_batch = y_batch.to(self.device)\n",
        "\n",
        "            mini_batch_loss = step_fn(x_batch, y_batch)\n",
        "            mini_batch_losses.append(mini_batch_loss)\n",
        "\n",
        "        loss = np.mean(mini_batch_losses)\n",
        "        return loss\n",
        "\n",
        "    # this function was updated in this class\n",
        "    def set_seed(self, seed=42):\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        random.seed(seed)\n",
        "        try:\n",
        "            self.train_loader.sampler.generator.manual_seed(seed)\n",
        "        except AttributeError:\n",
        "            pass\n",
        "\n",
        "    def train(self, n_epochs, seed=42):\n",
        "        # To ensure reproducibility of the training process\n",
        "        self.set_seed(seed)\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            # Keeps track of the numbers of epochs\n",
        "            # by updating the corresponding attribute\n",
        "            self.total_epochs += 1\n",
        "\n",
        "            # inner loop\n",
        "            # Performs training using mini-batches\n",
        "            loss = self._mini_batch(validation=False)\n",
        "            self.losses.append(loss)\n",
        "\n",
        "            # VALIDATION\n",
        "            # no gradients in validation!\n",
        "            with torch.no_grad():\n",
        "                # Performs evaluation using mini-batches\n",
        "                val_loss = self._mini_batch(validation=True)\n",
        "                self.val_losses.append(val_loss)\n",
        "\n",
        "    def save_checkpoint(self, filename):\n",
        "        # Builds dictionary with all elements for resuming training\n",
        "        checkpoint = {'epoch': self.total_epochs,\n",
        "                      'model_state_dict': self.model.state_dict(),\n",
        "                      'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                      'loss': self.losses,\n",
        "                      'val_loss': self.val_losses}\n",
        "\n",
        "        torch.save(checkpoint, filename)\n",
        "\n",
        "    def load_checkpoint(self, filename):\n",
        "        # Loads dictionary\n",
        "        checkpoint = torch.load(filename)\n",
        "\n",
        "        # Restore state for model and optimizer\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        self.total_epochs = checkpoint['epoch']\n",
        "        self.losses = checkpoint['loss']\n",
        "        self.val_losses = checkpoint['val_loss']\n",
        "\n",
        "        self.model.train() # always use TRAIN for resuming training\n",
        "\n",
        "    def predict(self, x):\n",
        "        # Set is to evaluation mode for predictions\n",
        "        self.model.eval()\n",
        "        # Takes aNumpy input and make it a float tensor\n",
        "        x_tensor = torch.as_tensor(x).float()\n",
        "        # Send input to device and uses model for prediction\n",
        "        y_hat_tensor = self.model(x_tensor.to(self.device))\n",
        "        # Set it back to train mode\n",
        "        self.model.train()\n",
        "        # Detaches it, brings it to CPU and back to Numpy\n",
        "        return y_hat_tensor.detach().cpu().numpy()\n",
        "\n",
        "    def count_parameters(self):\n",
        "      return sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "\n",
        "    def plot_losses(self):\n",
        "        fig = plt.figure(figsize=(10, 4))\n",
        "        plt.plot(self.losses, label='Training Loss', c='b')\n",
        "        plt.plot(self.val_losses, label='Validation Loss', c='r')\n",
        "        plt.yscale('log')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    @staticmethod\n",
        "    def _visualize_tensors(axs, x, y=None, yhat=None, layer_name='', title=None):\n",
        "        # The number of images is the number of subplots in a row\n",
        "        n_images = len(axs)\n",
        "        # Gets max and min values for scaling the grayscale\n",
        "        minv, maxv = np.min(x[:n_images]), np.max(x[:n_images])\n",
        "        # For each image\n",
        "        for j, image in enumerate(x[:n_images]):\n",
        "            ax = axs[j]\n",
        "            # Sets title, labels, and removes ticks\n",
        "            if title is not None:\n",
        "                ax.set_title(f'{title} #{j}', fontsize=12)\n",
        "            shp = np.atleast_2d(image).shape\n",
        "            ax.set_ylabel(\n",
        "                f'{layer_name}\\n{shp[0]}x{shp[1]}',\n",
        "                rotation=0, labelpad=40\n",
        "            )\n",
        "            xlabel1 = '' if y is None else f'\\nLabel: {y[j]}'\n",
        "            xlabel2 = '' if yhat is None else f'\\nPredicted: {yhat[j]}'\n",
        "            xlabel = f'{xlabel1}{xlabel2}'\n",
        "            if len(xlabel):\n",
        "                ax.set_xlabel(xlabel, fontsize=12)\n",
        "            ax.set_xticks([])\n",
        "            ax.set_yticks([])\n",
        "\n",
        "            # Plots weight as an image\n",
        "            ax.imshow(\n",
        "                np.atleast_2d(image.squeeze()),\n",
        "                cmap='gray',\n",
        "                vmin=minv,\n",
        "                vmax=maxv\n",
        "            )\n",
        "        return\n",
        "\n",
        "    def visualize_filters(self, layer_name, **kwargs):\n",
        "        try:\n",
        "            # Gets the layer object from the model\n",
        "            layer = self.model\n",
        "            for name in layer_name.split('.'):\n",
        "                layer = getattr(layer, name)\n",
        "            # We are only looking at filters for 2D convolutions\n",
        "            if isinstance(layer, nn.Conv2d):\n",
        "                # Takes the weight information\n",
        "                weights = layer.weight.data.cpu().numpy()\n",
        "                # weights -> (channels_out (filter), channels_in, H, W)\n",
        "                n_filters, n_channels, _, _ = weights.shape\n",
        "\n",
        "                # Builds a figure\n",
        "                size = (2 * n_channels + 2, 2 * n_filters)\n",
        "                fig, axes = plt.subplots(n_filters, n_channels,\n",
        "                                        figsize=size)\n",
        "                axes = np.atleast_2d(axes)\n",
        "                axes = axes.reshape(n_filters, n_channels)\n",
        "                # For each channel_out (filter)\n",
        "                for i in range(n_filters):\n",
        "                    Architecture._visualize_tensors(\n",
        "                        axes[i, :],\n",
        "                        weights[i],\n",
        "                        layer_name=f'Filter #{i}',\n",
        "                        title='Channel'\n",
        "                    )\n",
        "\n",
        "                for ax in axes.flat:\n",
        "                    ax.label_outer()\n",
        "\n",
        "                fig.tight_layout()\n",
        "                return fig\n",
        "        except AttributeError:\n",
        "            return\n",
        "\n",
        "    def attach_hooks(self, layers_to_hook, hook_fn=None):\n",
        "        # Clear any previous values\n",
        "        self.visualization = {}\n",
        "        # Creates the dictionary to map layer objects to their names\n",
        "        modules = list(self.model.named_modules())\n",
        "        layer_names = {layer: name for name, layer in modules[1:]}\n",
        "\n",
        "        if hook_fn is None:\n",
        "            # Hook function to be attached to the forward pass\n",
        "            def hook_fn(layer, inputs, outputs):\n",
        "                # Gets the layer name\n",
        "                name = layer_names[layer]\n",
        "                # Detaches outputs\n",
        "                values = outputs.detach().cpu().numpy()\n",
        "                # Since the hook function may be called multiple times\n",
        "                # for example, if we make predictions for multiple mini-batches\n",
        "                # it concatenates the results\n",
        "                if self.visualization[name] is None:\n",
        "                    self.visualization[name] = values\n",
        "                else:\n",
        "                    self.visualization[name] = np.concatenate([self.visualization[name], values])\n",
        "\n",
        "        for name, layer in modules:\n",
        "            # If the layer is in our list\n",
        "            if name in layers_to_hook:\n",
        "                # Initializes the corresponding key in the dictionary\n",
        "                self.visualization[name] = None\n",
        "                # Register the forward hook and keep the handle in another dict\n",
        "                self.handles[name] = layer.register_forward_hook(hook_fn)\n",
        "\n",
        "    def remove_hooks(self):\n",
        "        # Loops through all hooks and removes them\n",
        "        for handle in self.handles.values():\n",
        "            handle.remove()\n",
        "        # Clear the dict, as all hooks have been removed\n",
        "        self.handles = {}\n",
        "\n",
        "    def visualize_outputs(self, layers, n_images=10, y=None, yhat=None):\n",
        "        layers = filter(lambda l: l in self.visualization.keys(), layers)\n",
        "        layers = list(layers)\n",
        "        shapes = [self.visualization[layer].shape for layer in layers]\n",
        "        n_rows = [shape[1] if len(shape) == 4 else 1\n",
        "                  for shape in shapes]\n",
        "        total_rows = np.sum(n_rows)\n",
        "\n",
        "        fig, axes = plt.subplots(total_rows, n_images,\n",
        "                                figsize=(1.5*n_images, 1.5*total_rows))\n",
        "        axes = np.atleast_2d(axes).reshape(total_rows, n_images)\n",
        "\n",
        "        # Loops through the layers, one layer per row of subplots\n",
        "        row = 0\n",
        "        for i, layer in enumerate(layers):\n",
        "            start_row = row\n",
        "            # Takes the produced feature maps for that layer\n",
        "            output = self.visualization[layer]\n",
        "\n",
        "            is_vector = len(output.shape) == 2\n",
        "\n",
        "            for j in range(n_rows[i]):\n",
        "                Architecture._visualize_tensors(\n",
        "                    axes[row, :],\n",
        "                    output if is_vector else output[:, j].squeeze(),\n",
        "                    y,\n",
        "                    yhat,\n",
        "                    layer_name=layers[i] \\\n",
        "                              if is_vector \\\n",
        "                              else f'{layers[i]}\\nfil#{row-start_row}',\n",
        "                    title='Image' if (row == 0) else None\n",
        "                )\n",
        "                row += 1\n",
        "\n",
        "        for ax in axes.flat:\n",
        "            ax.label_outer()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def correct(self, x, y, threshold=.5):\n",
        "        self.model.eval()\n",
        "        yhat = self.model(x.to(self.device))\n",
        "        y = y.to(self.device)\n",
        "        self.model.train()\n",
        "\n",
        "        # We get the size of the batch and the number of classes\n",
        "        # (only 1, if it is binary)\n",
        "        n_samples, n_dims = yhat.shape\n",
        "        if n_dims > 1:\n",
        "            # In a multiclass classification, the biggest logit\n",
        "            # always wins, so we don't bother getting probabilities\n",
        "\n",
        "            # This is PyTorch's version of argmax,\n",
        "            # but it returns a tuple: (max value, index of max value)\n",
        "            _, predicted = torch.max(yhat, 1)\n",
        "        else:\n",
        "            n_dims += 1\n",
        "            # In binary classification, we NEED to check if the\n",
        "            # last layer is a sigmoid (and then it produces probs)\n",
        "            if isinstance(self.model, nn.Sequential) and \\\n",
        "              isinstance(self.model[-1], nn.Sigmoid):\n",
        "                predicted = (yhat > threshold).long()\n",
        "            # or something else (logits), which we need to convert\n",
        "            # using a sigmoid\n",
        "            else:\n",
        "                predicted = (F.sigmoid(yhat) > threshold).long()\n",
        "\n",
        "        # How many samples got classified correctly for each class\n",
        "        result = []\n",
        "        for c in range(n_dims):\n",
        "            n_class = (y == c).sum().item()\n",
        "            n_correct = (predicted[y == c] == c).sum().item()\n",
        "            result.append((n_correct, n_class))\n",
        "        return torch.tensor(result)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def loader_apply(loader, func, reduce='sum'):\n",
        "        results = [func(x, y) for i, (x, y) in enumerate(loader)]\n",
        "        results = torch.stack(results, axis=0)\n",
        "\n",
        "        if reduce == 'sum':\n",
        "            results = results.sum(axis=0)\n",
        "        elif reduce == 'mean':\n",
        "            results = results.float().mean(axis=0)\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "MQ27GjbyL80M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuração da arquitetura base (Modelo LeNet-like)"
      ],
      "metadata": {
        "id": "MBDM7dQpxmF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(13)\n",
        "lenet = nn.Sequential()\n",
        "\n",
        "# Featurizer\n",
        "# Block 1: 1@28x28 -> 6@28x28 -> 6@14x14\n",
        "lenet.add_module('C1', nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2))\n",
        "lenet.add_module('func1', nn.ReLU())\n",
        "lenet.add_module('S2', nn.MaxPool2d(kernel_size=2))\n",
        "# Block 2: 6@14x14 -> 16@10x10 -> 16@5x5\n",
        "lenet.add_module('C3', nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5))\n",
        "lenet.add_module('func2', nn.ReLU())\n",
        "lenet.add_module('S4', nn.MaxPool2d(kernel_size=2))\n",
        "# Block 3: 16@5x5 -> 120@1x1\n",
        "lenet.add_module('C5', nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5))\n",
        "lenet.add_module('func3', nn.ReLU())\n",
        "# Flattening\n",
        "lenet.add_module('flatten', nn.Flatten())\n",
        "\n",
        "# Classification\n",
        "# Hidden Layer\n",
        "lenet.add_module('dropout1', nn.Dropout(p=0.3))\n",
        "lenet.add_module('F6', nn.Linear(in_features=120, out_features=84))\n",
        "lenet.add_module('func4', nn.ReLU())\n",
        "lenet.add_module('dropout2', nn.Dropout(p=0.3))\n",
        "# Output Layer\n",
        "lenet.add_module('OUTPUT', nn.Linear(in_features=84, out_features=10))"
      ],
      "metadata": {
        "id": "6r6NfM8Xj_E0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparação dos dados\n"
      ],
      "metadata": {
        "id": "YAdXbfWCpil9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformedTensorDataset(Dataset):\n",
        "    def __init__(self, x, y, transform=None):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.x[index]\n",
        "\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "\n",
        "        return x, self.y[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)"
      ],
      "metadata": {
        "id": "Scv3owJUpi2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def index_splitter(n, splits, seed=13):\n",
        "    idx = torch.arange(n)\n",
        "    # Makes the split argument a tensor\n",
        "    splits_tensor = torch.as_tensor(splits)\n",
        "    total = splits_tensor.sum().float()\n",
        "    # If the total does not add up to one\n",
        "    # divide every number by the total\n",
        "    if not total.isclose(torch.ones(1)[0]):\n",
        "        splits_tensor = splits_tensor / total\n",
        "    # Uses PyTorch random_split to split the indices\n",
        "    torch.manual_seed(seed)\n",
        "    return random_split(idx, splits_tensor)\n",
        "\n",
        "def make_balanced_sampler(y):\n",
        "    # Computes weights for compensating imbalanced classes\n",
        "    classes, counts = y.unique(return_counts=True)\n",
        "    weights = 1.0 / counts.float()\n",
        "    sample_weights = weights[y.squeeze().long()]\n",
        "    # Builds sampler with compute weights\n",
        "    generator = torch.Generator()\n",
        "    sampler = WeightedRandomSampler(\n",
        "        weights=sample_weights,\n",
        "        num_samples=len(sample_weights),\n",
        "        generator=generator,\n",
        "        replacement=True\n",
        "    )\n",
        "    return sampler"
      ],
      "metadata": {
        "id": "px9lJ8GYpmnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregando o dataset MNIST (treinamento)\n",
        "mnist_dataset = torchvision.datasets.MNIST(root='./data', download=True, train=True)\n",
        "\n",
        "# Acessando imagens e labels diretamente\n",
        "imagens, labels = zip(*[(img, label) for img, label in mnist_dataset])"
      ],
      "metadata": {
        "id": "GIJ0TCOFqzJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base de dados\n",
        "Dados gerais da base de dados."
      ],
      "metadata": {
        "id": "5iUf97gwrPd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_dataset"
      ],
      "metadata": {
        "id": "F0El884IrMNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotagem das imagens\n",
        "Conjunto de dados formado por números de 0 a 9 em escala de cinza e classificados por label."
      ],
      "metadata": {
        "id": "2gUfZCVur0XJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "to_tensor = transforms.ToTensor()\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(8, 6))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    # Converter para tensor\n",
        "    img_tensor = to_tensor(imagens[i])\n",
        "\n",
        "    # Mostrar a imagem\n",
        "    ax.imshow(img_tensor.squeeze(), cmap='gray')\n",
        "\n",
        "    # Mostrar label + dimensão\n",
        "    ax.set_title(f\"Shape: {tuple(img_tensor.shape)} \\n Label: {labels[i]} \")\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bqbjvOH4rztx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_tensor = transforms.ToTensor()\n",
        "\n",
        "# Builds tensors from numpy arrays BEFORE split\n",
        "# Modifies the scale of pixel values from [0, 255] to [0, 1]\n",
        "x_tensor = torch.stack([to_tensor(img) for img in imagens]) # imagens é uma tupla\n",
        "y_tensor = torch.tensor(labels).long()\n",
        "\n",
        "# Uses index_splitter to generate indices for training and\n",
        "# validation sets\n",
        "train_idx, val_idx = index_splitter(len(x_tensor), [80, 20])\n",
        "# Uses indices to perform the split\n",
        "x_train_tensor = x_tensor[train_idx]\n",
        "y_train_tensor = y_tensor[train_idx]\n",
        "x_val_tensor = x_tensor[val_idx]\n",
        "y_val_tensor = y_tensor[val_idx]\n",
        "\n",
        "# We're not doing any data augmentation now\n",
        "train_composer = Compose([Normalize(mean=(0.1307,), std=(0.3081,))])\n",
        "val_composer = Compose([Normalize(mean=(0.1307,), std=(0.3081,))])\n",
        "\n",
        "# Uses custom dataset to apply composed transforms to each set\n",
        "train_dataset = TransformedTensorDataset(x_train_tensor, y_train_tensor, transform=train_composer)\n",
        "val_dataset = TransformedTensorDataset(x_val_tensor, y_val_tensor, transform=val_composer)\n",
        "\n",
        "# Builds a weighted random sampler to handle imbalanced classes\n",
        "sampler = make_balanced_sampler(y_train_tensor)\n",
        "\n",
        "# Uses sampler in the training set to get a balanced data loader\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=16, sampler=sampler)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=16)"
      ],
      "metadata": {
        "id": "6f_iJUIPpsGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criação do conjunto de dados de treinamento e teste"
      ],
      "metadata": {
        "id": "YhaurQRh5UTr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinamento"
      ],
      "metadata": {
        "id": "ccRbE_DKnCcs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "81KWi7vGra_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.001\n",
        "multi_loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
        "optimizer_cnn1 = optim.SGD(lenet.parameters(), lr=lr)\n",
        "arch_cnn1 = Architecture(lenet, multi_loss_fn, optimizer_cnn1)\n",
        "arch_cnn1.set_loaders(train_loader, val_loader)\n",
        "arch_cnn1.train(50)"
      ],
      "metadata": {
        "id": "r4LkGmMUnBuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cálculo das quantidades de acertos por classe\n",
        "train_correct = arch_cnn1.loader_apply(train_loader, arch_cnn1.correct)\n",
        "val_correct = arch_cnn1.loader_apply(val_loader, arch_cnn1.correct)\n",
        "\n",
        "# Soma total de acertos e total de amostras\n",
        "train_acc = train_correct[:,0].sum().item() / train_correct[:,1].sum().item()\n",
        "val_acc   = val_correct[:,0].sum().item() / val_correct[:,1].sum().item()\n",
        "\n",
        "print(\"Acurácia de treino:\", train_acc)\n",
        "print(\"Acurácia de validação:\", val_acc)\n"
      ],
      "metadata": {
        "id": "tajeAIZRufg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = arch_cnn1.plot_losses()"
      ],
      "metadata": {
        "id": "PmAjOw3xtO9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig_filters = arch_cnn1.visualize_filters('C1', cmap='gray')"
      ],
      "metadata": {
        "id": "ksDY-vdZSTX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hooks"
      ],
      "metadata": {
        "id": "J_qmg-itNtDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "featurizer_layers = ['C1', 'func1', 'S2', 'C3', 'func2', 'S4', 'C5', 'func3', 'flatten']\n",
        "classifier_layers = ['dropout1', 'F6', 'func4', 'dropout2', 'OUTPUT']\n",
        "\n",
        "arch_cnn1.attach_hooks(layers_to_hook=featurizer_layers + classifier_layers)\n",
        "\n",
        "images_batch, labels_batch = next(iter(val_loader))\n",
        "logits = arch_cnn1.predict(images_batch)\n",
        "predicted = np.argmax(logits, 1)\n",
        "\n",
        "arch_cnn1.remove_hooks()"
      ],
      "metadata": {
        "id": "CCptzpyQNu60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature maps"
      ],
      "metadata": {
        "id": "gh_m0CN4NulD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with plt.style.context('seaborn-v0_8-white'):\n",
        "    fig_maps1 = arch_cnn1.visualize_outputs(featurizer_layers[:3])\n",
        "    fig_maps2 = arch_cnn1.visualize_outputs(classifier_layers, y=labels_batch, yhat=predicted)"
      ],
      "metadata": {
        "id": "Q7tAMrLjNvTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "def plot_confusion_matrix(model, dataloader, device='cpu'):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Gera matriz de confusão\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm_normalized, annot=True, fmt=\".2f\", cmap=\"Blues\", xticklabels=range(10), yticklabels=range(10))\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "E3CxQSnEFHEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(lenet, val_loader)"
      ],
      "metadata": {
        "id": "vlNS0gpPFMmW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}